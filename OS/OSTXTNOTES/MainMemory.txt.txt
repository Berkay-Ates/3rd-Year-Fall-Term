              			MAIN MEMORY 
A typical instruction execution cycle for example first fetches an instruction from memory. The instruction is then decoded and may cause operands to be fetched from memeory. After the instruction has been executed on the operands results may be stored back in memory. The memory unit sees only a stream of memory addresses it does not know how they are generated or what they are for. 

BASIC HARDWARE 
Main memory and the registers built into the processor itself are the only general-purpose storage that the CPU can access directly. There are machine instructions that take memory addresses as arguments but none that take disk addresses. Therefore any instructions in execution and any data being used by the unstructions must be in one of these direct-access storage devices. IF the data are not in memory they msut be moved there before the CPU can operate them. 


Seperate per-process memory space protects the processes from each other and is fundamental to having multiple processes loaded in memory for concurrent execution. To determine which address space is assigned to the process we use 2 registers Base and Limit. The base register holds the smallest legal physical memory address the limit register specifies the size of the range. Protection of memory space is accomplished by having the CPU hardware compare every address generated in user mdoe with the registers.
Any attempt by a program executing in user mode to access OS memory or other users memory results in a trap to the operation system, which treats the atttempt as a fatal error. 

The base and limit registers can be loaded only by the operating system which uses a special privileged instruction. Since privileged instructions can be executed only in kernel mode, and since only the operating system executes in kernel mode only the OS can load the base and limit registers.  

The operating sytem executing in kernel mode is given unrestricted access to both operating-system memory and users memory. This provision allows the operating system to load users’ programs into users’ memory, to dump out those programs in case of errors, to access and modify parameters of system calls, to perform I/O to and from user memory, and to provide many other services

ADDRESS BINDING
The processes on the disk that are waiting to be brought into memory for executiong form the Input Queue. The normal singel-tasking proceure is to select one of the processes in the input queue and to load that process into memory. As the process is ececuted it accesses instructions and data from memory. Eventually the process terminates and its memory space is declared available. 

Most systems allow a user process to reside in any part of the physical memory. 
In most cases a user program goes through several steps. Addresses may be represented in defferent ways during these steps. Adresses in the source program are generally symbolic. A compiler typically BINDS these symbolic addresses to relocatable addresses. The linkage editor or loader in turn binds the relocatable addresses to absolute addresses. 

symbolic =>bind by compiler => relocatable => turn real address by linker or loader
symbolic is source kode. 

Clasically the binding of instructions and data to memory addresses can be done at any step along the way. Usually in these 3 steps. 

COMPILE TIME: If you know at compile time where the process will reside in memory, then ABSOLUTE CODE can be generated. For example if you know that a user process will reside starting at location R, then the generated compiler code will start a that location and extend up from there. If at some later time, the starting location changes, then it will be necessary to recompile this code. The MS-DOS .COM-format programs are bound at compile time. 

LOAD TIME:If it is not known at compile time where the process will reside in memory, then the compiler must generate RELOCATABLE CODE. In this case, final binding is delayed until load time. If the starting address changes we need only reload the user code to incororate this changed value. 

EXECUTION TIME: If the process can be moved during its execution from one memory segment to another then binding must be delayed until run time. Special hardware msut be available for this scheme to work. Most general-purpose operating systems use this method. 


kod =>compile > object module => linker > loadmodule => loader => in memory binary memory 

		 	   LOGICAL VERSUS PHYSICAL ADDRESS SPACE 
An address generated by the CPU is commonly referred to as a LOGICAL ADDRESS, whereas an address seen by the memory unit that is, the one loaded into the memory-address register of the memory is commonly reffered to asa a PHYSICAL ADDRESS. 
The compile-time and load-time address-binding methods generate identical logical and physical addresses. However the execution-time address binding results in deffering logical and physical address. In this case we usually refer to the logical address as a VIRTUAL ADDRESS. (LOGICAL ADDRESS ~~ VIRTUAL ADDRESS). 

The set of all logical addresses generated by a progrem is a LOGICAL ADDRESS SPACE. The set of all physical addresses corresponding to these logical addresses is a PHYSICAL ADDRESS SPACE. Thus in the execution time a dddress-binding-scheme, the logical and physical address spaces differ. 

The run-time mapping from virtual to physical addresses is done by a hardware device called the MEMORY-MANAGEMENT UNIT (MMU). The BASE REGISTER is now called a RELOCATON REGISTER. The value in the relocation register is added to every address genereated by a user process at the time the address is sent to memory. The user program never sees the real physical addresses. The user program deals with logical addresses. The memory mapping hardware converts logical addresses into physical addresses.

The user program generates only logical addresses and thinks that the process runs in locations 0 to max. However these logical addresses must be mapped to physical addresses before they are used. 


DYNAMIC LOADING 
To obtain better memory-space utilization we can use DYNAMIC LOADING. With dynamic loading a routine is not loadded until it is called. All routines are kept on disk in a relocatable load format. The main program is loaded into memory and is executed. When a routine needs to call another routine, the calling routine first checks to see whether the other routine has been loaded. If it has not, the relocatable linking loader is called to load the desired routine into memory and to update the programs's address tables to reflect this change. Then control is passed to the newly loaded routine. 

Dynamic loading does not requre special support from the operating system. It is the responsibility of the users to design their programs to take advantage of such a method. Operating systems may help the programmer however by providing library routines to implement dynamic loading. 

			  DYNAMIC LINKING AND SHARED LIBRARIES 
DYNAMIC LINKED LIBRARIES are system libraries that are linked to user programs when the programs are run. Some operating systems support only STATIC LINKING in which system libraries are treated like any other object module and are combined by the loader into the binary program image. Dynamic linking in contrast is similar to dynamic loading. Without this facility each program on a system must include a copy of its language library in the executable image. 

With dynamic linking, a stub is included in the image for each library routine reference. The stub is a small piece of code that indicates how to locate teh appropriate memory-resident library routine or how to load the library if the routine is not already present. Stubs loads the library if it is not loaded for once and then when program needs again the same library it do not waste time by dynamically loading the program again and again. 

Stub programi main memory de ortak alana yeni libraryi ekler bu sayede tum programlar ayni kopyayi kullanir. Eger libraryde bir degisiklik update olursa tum programlar da boylece en guncel libraryi kullanirlar. Dinamik Linking sayesinde farkli programlarin ayni librarileri kullanmasi kavramina ise SHARED LIBRARIES denir. 

Unlike dynamic loading, dynamic linking and shared libraries generally require help from the OS. If the processes in memory are protected from one another, then the operating system is the only entity that can check to see whether the needed routine is in another process’s memory space or that can allow multiple processes to access the same memory addresses.


				    SWAPPING 
A process can be SWAPPED temporarily out of memory to a backing store an then  brought back into memory for continued execution. Swapping makes it possible for the total physical address space of all process to execeed the real physical memory of the system, thus increasing the degree of multiprogramming in a system. 

STANDARD SWAPPING 
Standart swapping involves moving processes betwewen main memory and a backing store.The backing store is commonly a fast disk. The system maintains a READY QUEUE consisting of all processes whose memory images are on the backing store or in memory and are ready to run. Whenever the CPU scheduler decides to execute process it calls dispatcher. THe dispatcher checks to see whether the next process in the queue is in memory. If it is not and if there is no free memory region the dispatcher swaps out a process currently in memory and swaps in the desired process. It then reloads registers and transfers control to the selected process. 

The major part of swap time is transfer time; total transfer time is directly proportional to the amount of memory swapped.

There are two main solutions to swapping out a process that is waiting for I/O. First one is never swap a process with pendign IO or execute IO operations only into OS buffers. Transfers between OS buffers and process memeory then occur only when the process is swapped in. Note that this double buffering itself adds overhead. WE now need to copy the data again from kernel memory to user memory before the user process can access it. 

Standard swapping is not used in modern OS. Modified version of the Swapping used in modern OS. Moreover if the free memory is enough then the swapping disables int these OS. 

			      SWAPPING ON MOBILE SYSTEMS
Although most operating systems for PCs and servers support some modified version of swapping mobile systems typically do not support swapping in any form. Mobile devices generally use flash memory rather than more spacious hard disks as their persistent storage. The resulting space constraint is one reasong why mobile OS designers avoid swapping. Other reasons include the limited nubmer of writers  that flash memory can tolerate before it becomes unreliable and the poor throughtput become main memory and flash memory in these devices. 


			CONTIGUOUS MEMORY ALLOCATIONS 
The memory is usually divided into two partitions one for the resident  OS and one for the user processes. WE can place the operating system in either low memory or high memory. The major factor affecting this decision is the location of the interrupt vector. Since the interrupt vector is often in low memory programmers usually place OS in low memory as well. We usually want several user processes to reside in memory at the same time. We therefore need to consider how to allocate available memory to the processes that are in the input queue waiting to be brought into meemory. In contigous memory allocation each process is contained in a single section of memory that is contiguus to the section containing the next process. 

				MEMORY PROTECTION 
We can prevent a process from accessing memory it does not own by combining two ideas previously discussed. If we have a  relocations register together with a limit register we accomplish our goal. When the CPU scheduler selects a process for execution the dispatcher loads the relocation and limit registers with the correct values as part of the context switch. 

If a device driver (or other operating-system service) is not commonly used, we do not want to keep the code and data in memory, as we might be able to use that space for other purposes. Such code is sometimes called TRANSIENT operating-system code; it comes and goes as needed. Thus, using this code changes the size of the operating system during program execution.

				MEMORY ALLCOATION 
One of the simplest methods for allocating memory is to divide memory into several FIXED-SIZED partitions. Each partition may contain exactly one procss. Thus the degree of multiprogramming is bound by the number of partitions. In this MULTIPLE PARTITION METHOD, when a partition is free a process is selected from the input queue and is loaded into the free partition. When the process terminated the partition becomes available for another process. 

In the variable-partition scheme, the operating system keeps a table indicating which parts of memory are available and which are occupied. Initially, all memory is available for user processes and is considered one large block of available memory, a hole. Eventually, as you will see, memory contains a set of holes of various sizes.

As processes enter the system, they are put into an input queue. The operating system takes into account the memory requirements of each process and the amount of available memory space in determining which processes are allocated memory. When a process is allocated space, it is loaded into memory, and it can then compete for CPU time. When a process terminates, it releases its memory, which the operating system may then fill with another process from the input queue. In general as mentioned the memory blocks available comprise a set of holes of various sizes scattered throughout memory. 

If a released memory is adjacent to other holes these adjacent holes are merged to form one larger hole. At this point, the system may need to check whether there are processes waiting for memory and whether this newly freed and recombined memory could satisfy the demands of any of these waiting processes. 

This procedure is a particular instance of the general dynamic storageallocation problem, which concerns how to satisfy a request of size n from a list of free holes. There are many solutions to this problem. The FIRST-FIRST, BEST-FIT, and WORST-FIT strategies are the ones most commonly used to select a free hole from the set of available holes.


First fit. Allocate the first hole that is big enough. Searching can start either at the beginning of the set of holes or at the location where the previous first-fit search ended. We can stop searching as soon as we find a free hole that is large enough.

Best fit. Allocate the smallest hole that is big enough. We must search the entire list, unless the list is ordered by size. This strategy produces the smallest leftover hole.

Worst fit. Allocate the largest hole. Again, we must search the entire list, unless it is sorted by size. This strategy produces the largest leftover hole, which may be more useful than the smaller leftover hole from a best-fit approach.


				    FRAGMENTATION
As processes are loaded and removed from memory, the free memory space is broken into little pieces. EXTERNAL FRAGMENTATION exists when there is enough total memory space to satisfy a request but the available spaces are not contiguous: storage is fragmented into a large number of small holes.

Statistical analysis of first fit, for instance, reveals that, even with some optimization,
given N allocated blocks, another 0.5 N blocks will be lost to fragmentation. That is, one-third of memory may be unusable! This property is known as the 50-percent rule.

Memory fragmentation can be internal as well as external. Consider a multiple-partition allocation scheme with a hole of 18.464 bytes. Suppose that the next process request 18.462 bytes. If we allocate exactly the requested block we are left with a hole of 2 bytes. The general approach to avoiding this problem is to break the physical memory into fixed-sized blocks and allocate memory in units based on block size. The unused space in a single fragment is INTERNAL FRAGMENTATION --unused memory that is internat to a partition. 

One solution to the problem of external fragmentation is COMPACTION. The goal is to shuffle the memory contents so as to place all free memory together in one large block. Compaction is not always possible, however. If relocation is static and is done at assembly or load time, compaction cannot be done. It is possible only if relocation is dynamic and is done at execution time. If addresses are relocated dynamically, relocation requires only moving the program and data and then changing the base register to reflect the new base address.

Another possible solution to the external-fragmentation problem is to permit the logical address space of the processes to be noncontiguous, thus allowing a process to be allocated physical memory wherever such memory is available. Two complementary techniques achieve this solution: SEGMENTATION and PAGING.

				SEGMENTATION 
Dealing with memory in terms of its physical properties is inconvenient to both the operating system and the programmer. What if the hardware could provide a memory mechanism that mapped the programmer’s view to the actual physical memory? The system would have more freedom to manage memory, while the programmer would have a more natural programming environment. Segmentation provides such a mechanism. SEGMENTATION is a memory-management scheme that supports this programmer view of memory which is kind of a view like the objects located somewere in the space. A logical address space is a collection of segments. 
Each segment has a name and a length. The addresses specify both the segment name and the offset within the segment. The programmer therefore specifies each address by two quantities: a segment name and an offset. 

=> Segmentation logical addresin segmentlere ayrilmasi ve programcinin hizmetine sunulmasina yonelik bir yaklasimdir.

Normally, when a program is compiled, the compiler automatically constructs segments reflecting the input program.
C compiler might create separate segments for the following:
1. The code
2. Global variables
3. The heap, from which memory is allocated
4. The stacks used by each thread
5. The standard C library


SEGMENTATION HARDWARE
 Although the programmer can now refer to objects in the program by a two-dimensional address, the actual physical memory is still, of course, a one dimensional sequence of bytes. Thus, we must define an implementation to map two-dimensional user-defined addresses into one-dimensional physical addresses. This mapping is effected by a segment table. Each entry in the segment table has a segment base and a segment limit. The segment base
contains the starting physical address where the segment resides in memory, and the segment limit specifies the length of the segment.

SEGMENT-TABLE BASE REGISTER (STBR) points to the segment table's location in memory. 
SEGMENT-TABLE LENGTH REGISTER (STLR) indicates number of segments uded by a program 

A logical address consists of two parts: a segment number, s, and an offset into that segment, d. The segment number is used as an index to the segment table. The offset d of
the logical address must be between 0 and the segment limit. If it is not, we trap to the operating system (logical addressing attempt beyond end of segment). When an offset is legal, it is added to the segment base to produce the address in physical memory of the desired byte. The segment table is thus essentially an array of base–limit register pairs.

					
				PAGING 
Segmentation permits the physical address space of a proces to be noncontiguous. PAGING is another memory-management scheme that offers this advantage. However paging avoids external fragmentation and the need for compaction, whreas segmentation does not. 

The basic method for implementing paging involves breaking physical memory into fixed-sized blocks called frames and breaking logical memory into blocks of the same size called pages. When a process is to be executed, its pages are loaded into any available memory frames from their source (a file system or the backing store). The backing store is divided into fixed-sized blocks that are the same size as the memory frames or clusters of multiple frames. This rather simple idea has great functionality and wide ramifications.

Every address generated by the CPU is divided into two parts: A PAGE NUMBER (P) and A PAGE OFFSET (D). The page number is used as an index into a PAGE TABLE. The page table contains the base address of each page in physical memory. This base address is combined with the page offsest to define the physical memory address that is sent to the memory unit. 

p,d => page table dan p nin gosterdigi addrese bakariz bu deger F olsun (P+D=logical addr)
F,d => F ile d yi birlestirerek physical memory addresine erisiriz. 
F bizim base address degerimiz. 

The page size is defined by the hardware. The size of a page is a power of 2, varying betweeen 5122 bytes and 1GB.

If the logical address space is 2^m, and a page size is 2^n bytes. then the higher order m-n bits of a logical address designate the page number which is p, and the n low order bits designate the page offset.  

You may have noticed that paging itself is a form of dynamic relocation. Every logical address is bound by the paging hardware to some physical address. Using paging is similar to using a table of base registers one for each fram of memory. 

When a process arrives in the system to be executed, its size, expressed in pages, is examined. Each page of the process needs one frame. Thus, if the process requires n pages, at least n frames must be available in memory. If n frames are available, they are allocated to this arriving process. The first page of the process is loaded into one of the allocated frames, and the frame number is put in the page table for this process. The next page is loaded into another frame, its frame number is put into the page table, and so on

Notice that the user process by definition is unable to access memory it does not own. It has no way of addressing memory outside of its page table, and the table includes only those pages that the process owns.

Since OS is managing physical memory it must be aware of the allocation details of physical memory. This information is generally kept in a data structure called a FRAME TABLE. The FRAME TABLE has one entry for eacch physical page frame, indicating whether the latter is free or allocated and if it is allocated to which page of which process or processes.  



				    HARDWARE SUPPORT
The hardware implementation of the page table can be done in several ways. In the simplest case, the page table is implemented as a set of dedicated registers. Most contemporary computers, however allow the page table to be very large. For these machines the use of fast registers to implement the page table is not feasible. Tathere the page table is kept in main memory and a PAGE-TABLE BASE REGISTER (PTBR) points to the page table. Changing page tables requires changing only this one register, substantially reducing context-switch time. This problem with tis approach is the time required to access a user memory location.

The standart solution to this problem is to use a special small fast lookup harware cache called TRANSLATION LOOK-ASIDE BUFFER (TLB). 


If we want to access location i, we must first index into
the page table, using the value in the PTBR offset by the page number for i. This
task requires a memory access. It provides us with the frame number, which
is combined with the page offset to produce the actual address. We can then
access the desired place in memory. With this scheme, two memory accesses
are needed to access a byte (one for the page-table entry, one for the byte). Thus,
memory access is slowed by a factor of 2.


The standard solution to this problem is to use a special, small, fast lookup hardware cache called a translation look-aside buffer (TLB). The TLB
is associative, high-speed memory. Each entry in the TLB consists of two parts:
a key (or tag) and a value. When the associative memory is presented with an
item, the item is compared with all keys simultaneously. If the item is found,
the corresponding value field is returned. The search is fast; a TLB lookup in
modern hardware is part of the instruction pipeline, essentially adding no
performance penalty. To be able to execute the search within a pipeline step,
however, the TLB must be kept small. It is typically between 32 and 1,024 entries
in size.


 If the TLB is already full of entries, an existing entry must be selected for replacement. Replacement policies range from least recently used (LRU) through round-robin to random. Some CPUs allow the operating system to participate in LRU entry replacement, while others handle the matter themselves. Furthermore, some TLBs allow certain entries to be wired down, meaning that they cannot be removed from the TLB. Typically, TLB entries for key kernel code are wired down.

Some TLBs store address-space identifiers (ASIDs) in each TLB entry. An
ASID uniquely identifies each process and is used to provide address-space
protection for that process. When the TLB attempts to resolve virtual page
numbers, it ensures that the ASID for the currently running process matches the
ASID associated with the virtual page. If the ASIDs do not match, the attempt is
treated as a TLB miss. In addition to providing address-space protection, an ASID
allows the TLB to contain entries for several different processes simultaneously
If the TLB does not support separate ASIDs, then every time a new page table
is selected (for instance, with each context switch), the TLB must be flushed
(or erased) to ensure that the next executing process does not use the wrong
translation information. Otherwise, the TLB could include old entries that
contain valid virtual addresses but have incorrect or invalid physical addresses
left over from the previous process

The percentage of times that the page number of interest is found in the
TLB is called the hit ratio.

with 80% hit ratio:
To find the effective memory-access time, we weight the case by its probability:
effective access time = 0.80 × 100 + 0.20 × 200 = 120 nanoseconds
In this example, we suffer a 20-percent slowdown in average memory-access time (from 100 to 120 nanoseconds).

For a 99-percent hit ratio, which is much more realistic, we have
effective access time = 0.99 × 100 + 0.01 × 200 = 101 nanoseconds


       				    PROTECTION 
Memory protection in a paged environment is accomplished by protection bits
associated with each frame. Normally, these bits are kept in the page table.
One bit can define a page to be read–write or read-only. Every reference
to memory goes through the page table to find the correct frame number. At
the same time that the physical address is being computed, the protection bits
can be checked to verify that no writes are being made to a read-only page. An
attempt to write to a read-only page causes a hardware trap to the operating
system (or memory-protection violation).

One additional bit is generally attached to each entry in the page table: a
valid–invalid bit. When this bit is set to valid, the associated page is in the
process’s logical address space and is thus a legal (or valid) page. When the
bit is set to invalid, the page is not in the process’s logical address space. Illegal
addresses are trapped by use of the valid–invalid bit. The operating system
sets this bit for each page to allow or disallow access to the page.

Some systems provide hardware, in the form of a page-table
length register (PTLR), to indicate the size of the page table. This value is
checked against every logical address to verify that the address is in the valid
range for the process. Failure of this test causes an error trap to the operating
system.


 				SHARED PAGES
An advantage of paging is the possibility of sharing common code. This consideration is particularly important in a time-sharing environment. Consider a
system that supports 40 users, each of whom executes a text editor. If the text
editor consists of 150 KB of code and 50 KB of data space, we need 8,000 KB to
support the 40 users. If the code is reentrant code (or pure code), however, it
can be shared.To be sharable, the code must be reentrant. The read-only nature of shared code should not be left to the correctness of the code; the operating system should enforce this property.


				STRUCTURE OF THE PAGE TABLE 

HIERARCHICAL PAGING 
Most modern computer systems support a large logical address space
(232 to 264). In such an environment, the page table itself becomes excessively
large. For example, consider a system with a 32-bit logical address space. If
the page size in such a system is 4 KB (212), then a page table may consist of
up to 1 million entries (232/212).

One simple solution to this problem is to divide the page table
into smaller pieces. We can accomplish this division in several ways.
One way is to use a two-level paging algorithm, in which the page table
itself is also paged. For example, consider again the system with
a 32-bit logical address space and a page size of 4 KB. A logical address is
divided into a page number consisting of 20 bits and a page offset consisting
of 12 bits. Because we page the page table, the page number is further divided into a 10-bit page number and a 10-bit page offset. Thus a logical address is as follows where p1 is an index into the outer page table and p2 is the displacement
within the page of the inner page table.

		page number       page offset	
		P1	P2	     D
 		10	10	     12 

	
The VAX is a 32-
bit machine with a page size of 512 bytes. The logical address space of a process
is divided into four equal sections, each of which consists of 230 bytes. Each
section represents a different part of the logical address space of a process. The
first 2 high-order bits of the logical address designate the appropriate section.
The next 21 bits represent the logical page number of that section, and the final
9 bits represent an offset in the desired page. 



HASHED PAGE TABLES (SEPERATE CHANING MANTIGI)
A common approach for handling address spaces larger than 32 bits is to use
a hashed page table, with the hash value being the virtual page number. Each
entry in the hash table contains a linked list of elements that hash to the same
location (to handle collisions). Each element consists of three fields: (1) the
virtual page number, (2) the value of the mapped page frame, and (3) a pointer
to the next element in the linked list.
The algorithm works as follows: The virtual page number in the virtual
address is hashed into the hash table. The virtual page number is compared
with field 1 in the first element in the linked list. If there is a match, the
corresponding page frame (field 2) is used to form the desired physical address.
If there is no match, subsequent entries in the linked list are searched for a
matching virtual page number.

A single page-table entry can store the mappings for multiple physical-page frames. Clustered page tables are particularly useful for sparse address spaces, where memory
references are noncontiguous and scattered throughout the address space.


INVERTED PAGE TABLES 
An inverted page table has one entry for each real page (or frame) of memory. Each entry consists of the virtual address of the page stored in that real memory location, with information about the process that owns the page. Thus, only one page table is in the system, and it has only one entry for each page of physical
memory.


Inverted page tables often require that an address-space identifier be
stored in each entry of the page table, since the table usually contains several
different address spaces mapping physical memory. Storing the address-space
identifier ensures that a logical page for a particular process is mapped to the
corresponding physical page frame. 
<process-id, page-number, offset>.

Because the inverted page table is sorted by physical
address, but lookups occur on virtual addresses, the whole table might need
to be searched before a match is found. This search would take far too long.
To alleviate this problem, we use a hash table, to limit the search to one—or at most a few—page-table entries.


Systems that use inverted page tables have difficulty implementing shared
memory. Shared memory is usually implemented as multiple virtual addresses
(one for each process sharing the memory) that are mapped to one physical
address. This standard method cannot be used with inverted page tables;
because there is only one virtual page entry for every physical page, one
physical page cannot have two (or more) shared virtual addresses. A simple
technique for addressing this issue is to allow the page table to contain only
one mapping of a virtual address to the shared physical address. This means
that references to virtual addresses that are not mapped result in page faults.





























